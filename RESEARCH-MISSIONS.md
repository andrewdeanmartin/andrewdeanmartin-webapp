# Research Missions — Parallel Agent Briefs

> Each mission below is a self-contained research brief. Launch one agent per mission. They can run simultaneously. Each produces a structured output that feeds directly into website demos, content, and positioning.

---

## Mission 1: Enterprise Multi-Agent AI — Real Problems, Real Failures, Real Wins

### Research Question
What are the actual enterprise problems that multi-agent AI pipelines are being used to solve in 2025–2026? What's working, what's failing, and what are the specific pain points that make organizations reach out for help?

### Why This Matters
Feeds the **Agent Pipeline Visualizer** demo. The scenarios in the demo need to be grounded in real problems that David (Partner), Sarah (CDO), and Marcus (Alliance) actually encounter — not hypothetical use cases.

### What to Find

**Working implementations (case studies and examples):**
- Multi-agent systems in CRM migration or platform modernization
- Agentic AI in regulated industries (pharma, financial services, healthcare)
- Human-in-the-loop orchestration in enterprise workflows
- Agent pipelines for compliance validation, content generation, or document processing
- Named companies, vendors, or consulting firms doing this at scale

**Failure modes and pain points:**
- Why enterprise AI agent projects fail (not pilot failures — production failures)
- Common governance/compliance blockers for agentic systems
- The "last mile" problem: getting agent outputs into production systems
- Cost and latency issues with multi-agent orchestration at enterprise scale
- Security and data leakage concerns with agent pipelines

**Market data:**
- Market size and growth for enterprise agentic AI (2025–2026 data)
- Adoption rates: what percentage of enterprises are using multi-agent systems vs. single-model approaches
- Investment trends: where enterprise AI budgets are going
- Analyst reports (Gartner, Forrester, McKinsey, BCG) on agentic AI maturity

**Specific problem statements people are posting about:**
- LinkedIn posts, Reddit threads, Hacker News discussions where enterprise leaders describe their agent pipeline challenges
- Conference talks (re:Invent, Google Next, Salesforce Dreamforce, Microsoft Ignite) about multi-agent architecture
- Open questions the industry hasn't answered yet

### Output Format
Structured findings document with:
- 5–8 real problem statements (quoted or paraphrased, with source)
- 3–5 case studies or named implementations
- Key statistics with citations
- 3–5 failure patterns with explanations
- The "gap" — what's missing from the current market that Andrew's demo can address

---

## Mission 2: AI Governance in Enterprise — The Compliance Bottleneck

### Research Question
What are the specific governance, compliance, and risk management challenges that block enterprise AI projects from reaching production? What does "governance without bureaucracy" actually look like in practice?

### Why This Matters
Feeds the **Agent Pipeline Visualizer** (human-in-the-loop checkpoint design), the **website's positioning** ("governance without bureaucracy" is a core brand claim), and **LinkedIn content** (governance post). Sarah (CDO) ranks this as her #1 concern. This research makes the brand claim evidence-based.

### What to Find

**The compliance bottleneck — real data:**
- What percentage of enterprise AI projects stall or die in compliance review?
- Average time from AI pilot to production approval
- Which regulations are creating the most friction (EU AI Act, HIPAA, SOX, GxP, FDA)?
- Cost of governance delays to organizations

**What "good" governance looks like:**
- Frameworks being adopted (NIST AI RMF, ISO 42001, internal corporate frameworks)
- Companies that have successfully built AI governance into their development lifecycle (not bolted on after)
- The concept of "governance by design" vs. "governance by review" — who's writing about this?
- Human-in-the-loop implementation patterns in production systems

**What's failing:**
- Examples of AI projects blocked by compliance teams
- Examples of AI incidents that happened because governance was skipped
- The tension between "move fast" and "stay compliant" in enterprise AI
- What compliance teams actually want from AI development teams (interviews, surveys, reports)

**Regulatory landscape (current):**
- EU AI Act implementation status and enterprise impact (2025–2026)
- US regulatory landscape for AI in financial services, healthcare, pharma
- Industry-specific compliance requirements for AI (GxP, HIPAA, SOX)
- What's coming next — proposed regulations that will affect enterprise AI

**Tools and platforms:**
- AI governance tools and platforms (Credo AI, Holistic AI, IBM OpenPages, etc.)
- How Salesforce, Microsoft, Google are building governance into their AI platforms
- Open-source governance frameworks and toolkits

### Output Format
Structured findings document with:
- 5–8 specific governance pain points (with data/sources)
- The compliance timeline problem: average time from pilot to production
- 3–5 examples of governance done well
- 3–5 examples of governance failures or blocks
- Regulatory summary: what's in force and what's coming
- The "gap" — where governance is hardest and where Andrew's approach adds value

---

## Mission 3: Technical Due Diligence & Codebase Assessment — What CTOs Actually Need

### Research Question
When a CTO or technical leader evaluates a codebase for production readiness, acquisition, or modernization — what are they actually looking for? What are the standard approaches, and where do they fall short?

### Why This Matters
Feeds the **Codebase Health Assessment** demo (8-workstream methodology). This research validates the workstreams, ensures they match what technical leaders actually care about, and identifies gaps that make Andrew's approach distinctive.

### What to Find

**How technical due diligence is done today:**
- Standard approaches for codebase assessment in M&A, PE, and enterprise modernization
- What firms (consulting, PE, VC) look for when evaluating a tech asset
- Common frameworks: DORA metrics, SPACE framework, code quality tools (SonarQube, CodeClimate)
- The difference between automated scanning and expert assessment

**The 8 workstreams — validation and enhancement:**
- Architecture assessment best practices (microservices maturity, coupling, API design)
- Code quality metrics that matter (beyond basic linting)
- Security assessment approaches (OWASP, SAST/DAST, dependency scanning)
- Dependency health (supply chain risk, outdated packages, license compliance)
- Testing maturity (coverage, pyramid balance, integration vs. unit)
- Performance assessment (load testing, profiling, scalability patterns)
- Technical debt quantification (tools, approaches, financial models)
- Documentation assessment (what matters, what doesn't)

**What CTOs/technical leaders actually complain about:**
- Pain points with existing assessment approaches
- What they wish they got from a technical assessment
- The "executive summary" problem: translating technical findings for non-technical stakeholders
- Common surprises found in codebase assessments (patterns that recur across industries)

**Market context:**
- Technical due diligence market (M&A, PE portfolio, enterprise modernization)
- How AI is changing codebase assessment (AI-augmented code review, automated architecture analysis)
- Tools: what exists and what's missing

**AI-augmented assessment specifically:**
- How teams are using LLMs/AI to augment codebase reviews
- Accuracy and reliability of AI-generated code analysis
- Best practices for combining AI analysis with human expertise
- Limitations and risks of AI-only assessment

### Output Format
Structured findings document with:
- Validation of the 8 workstreams (confirmed, adjusted, or expanded)
- 5–8 specific pain points CTOs have with current assessment approaches
- The "executive translation" problem — data on the gap between technical findings and business decisions
- Tools landscape (what exists, what's AI-augmented, what's missing)
- 3–5 real examples of technical due diligence outcomes (anonymized if needed)
- The "gap" — where existing approaches fail and where Andrew's methodology fills it

---

## Mission 4: Enterprise Prompt Engineering — Beyond the Basics

### Research Question
What does enterprise-grade prompt engineering actually look like in 2025–2026? What problems do large organizations face when trying to get consistent, reliable, governed outputs from LLMs — and what frameworks exist?

### Why This Matters
Feeds **SharpPrompt Enterprise** demo. The demo needs to solve a real problem that enterprise teams face — not just "write a better prompt" but "get consistent, auditable, repeatable AI outputs across a 500-person organization."

### What to Find

**The enterprise prompt engineering problem:**
- How large organizations manage prompt quality and consistency at scale
- The "prompt sprawl" problem: different teams using different approaches with no standards
- Prompt versioning, testing, and governance in enterprise settings
- Cost implications of poorly structured prompts (token waste, retry cycles, hallucination costs)

**Existing frameworks and tools:**
- Prompt engineering frameworks in use at enterprise scale (DSPy, LMQL, guidance, etc.)
- Enterprise prompt management platforms (PromptLayer, Humanloop, Weights & Biases, etc.)
- How companies like Salesforce, Microsoft, and Google structure prompts in their AI products
- Academic research on prompt structure and effectiveness

**What's working and what isn't:**
- Case studies of enterprise prompt engineering programs
- Common failure modes (inconsistent outputs, hallucination, context window misuse)
- The role of "prompt templates" vs. "prompt libraries" vs. "prompt catalogs"
- How teams train non-technical staff to write effective prompts

**The Role → Task → Context → Requirements → Output → Input methodology:**
- Does this structure align with current research on effective prompt patterns?
- What other structures are in use (chain-of-thought, few-shot, role-play, etc.)?
- Academic or industry validation of structured prompt formats
- Where does structure help and where does it constrain?

**Enterprise-specific use cases:**
- Prompt engineering for regulated outputs (compliance documentation, audit reports)
- Prompt engineering for multi-agent systems (inter-agent prompting)
- Prompt engineering for code generation in enterprise settings
- Prompt engineering for data analysis and reporting

### Output Format
Structured findings document with:
- The enterprise prompt engineering problem statement (with data)
- 5–8 specific pain points organizations face
- Existing frameworks: what works, what doesn't, what's missing
- Validation (or critique) of the Role → Task → Context → Requirements → Output → Input approach
- Enterprise use cases that SharpPrompt Enterprise should address
- The "gap" — what no existing tool or framework solves well

---

## Mission 5: What Makes Senior Technical Leaders Reach Out — Conversion Triggers

### Research Question
When a Partner, CDO, CTO, or recruiter encounters a personal website or LinkedIn profile and decides to reach out — what specifically triggered that action? What content, proof, or experience on a site converts a passive viewer into an active contact?

### Why This Matters
Feeds **overall website conversion strategy**. We can build technically impressive demos, but if they don't trigger the specific "I should talk to this person" response in our personas, they're vanity projects.

### What to Find

**B2B conversion research:**
- What triggers a B2B decision-maker to initiate contact after visiting a website or profile (2025–2026 data)?
- The role of interactive content vs. static content in B2B lead generation
- "Dark social" — how much B2B engagement happens through referrals and DMs that never show up in analytics?
- Average time from first site visit to outreach in professional services contexts

**Executive and recruiter behavior:**
- How long do senior executives spend on a personal website before deciding to engage or leave?
- What elements on a consultant's or leader's site are most correlated with outreach?
- How do recruiters evaluate interactive demos vs. static portfolios?
- The role of "social proof" vs. "demonstrated capability" in triggering outreach

**Interactive content performance:**
- Do interactive tools/demos on personal or professional sites increase engagement and outreach?
- Case studies of consultants, engineers, or leaders who embedded tools in their sites and measured impact
- Interactive content benchmarks: time on page, scroll depth, return visits
- The "try before you buy" psychology in professional services

**What specifically makes consulting/AI leaders magnetic:**
- Examples of personal sites or profiles that generate significant inbound (named people if possible)
- Content formats that perform best for thought leadership conversion (posts vs. articles vs. videos vs. tools)
- The "retellable moment" — what makes someone screenshot your site and share it with a colleague?

**What doesn't work:**
- Common personal site elements that look impressive but don't convert
- Interactive demos that become gimmicks rather than trust-builders
- The "cool but useless" trap in B2B personal branding

### Output Format
Structured findings document with:
- 5–8 specific conversion triggers for B2B/executive audiences
- Data on interactive content effectiveness
- 3–5 examples of personal sites/profiles that generate inbound
- The "retellable moment" framework — what makes content shareable in professional contexts
- Anti-patterns: what looks cool but doesn't convert
- Specific recommendations for Andrew's site based on findings

---

## Mission 6: The CRM Migration Problem — Deep Dive for the Hero Demo

### Research Question
What are the specific, real-world challenges of enterprise CRM migration (Salesforce, Dynamics 365, Veeva, legacy systems)? This is the anchor scenario for the Agent Pipeline Visualizer demo — it needs to feel real.

### Why This Matters
The hero demo's "Migration Analysis" scenario needs to use real terminology, real process steps, and address real pain points. If the demo feels like someone who's never done a migration designed it, every persona will notice.

### What to Find

**The migration landscape:**
- How large enterprises currently approach CRM migration (Salesforce → Salesforce, Dynamics → Salesforce, legacy → modern, etc.)
- Average timeline and cost for enterprise CRM migration
- Common tools and platforms used in migration (MuleSoft, Informatica, Jitterbit, custom)
- The role of consulting firms in CRM migration

**Specific pain points:**
- Data mapping and transformation challenges
- Customization analysis (which custom objects, workflows, and automations carry over?)
- Integration complexity (how many systems does the CRM connect to?)
- User adoption post-migration
- Testing and validation at scale
- Compliance and regulatory requirements during migration (GxP, data residency, audit trail)

**Where AI/agents could help (and where they do today):**
- Existing AI tools for CRM migration analysis
- Where in the migration process are AI agents most useful (analysis, mapping, testing, documentation)?
- Salesforce's own AI migration tools (MuleSoft, Agentforce)
- Microsoft's migration tooling and AI integration
- The gap between what exists and what a multi-agent pipeline could do

**The human element:**
- Where human judgment is non-negotiable in CRM migration
- The "business rules" problem: implicit knowledge that lives in people's heads, not in code
- Change management during migration
- Stakeholder alignment challenges

### Output Format
Structured findings document with:
- CRM migration process map (stages and decision points)
- 5–8 specific pain points (with quotes/data from practitioners)
- Where AI/agents fit in each stage
- Where human-in-the-loop is essential (for the demo's checkpoint design)
- Real terminology and artifacts (object maps, field mappings, test matrices, compliance checklists)
- Enough detail that the demo scenario feels like it was built by someone who's done the work

---

## Mission 7: The Director → VP Career Market — What's Out There

### Research Question
What does the current market look like for Director/VP-level roles in AI delivery, digital transformation, and technology consulting? What are the specific titles, compensation ranges, and skill requirements?

### Why This Matters
Feeds **LinkedIn optimization** and **overall positioning**. Understanding what Rachel the recruiter is actually searching for — the exact titles, keywords, and requirements in active job specs — ensures Andrew's profile appears in the right searches and his site speaks to the right opportunities.

### What to Find

**Active market for AI delivery leaders:**
- Current VP/SVP/Director roles in AI, digital transformation, and technology delivery (title patterns)
- Which industries are hiring most aggressively for this profile?
- Consulting firm vs. enterprise (industry) vs. PE/portfolio company — where's the demand?
- Remote vs. location-specific requirements

**Job spec analysis:**
- 10–15 actual job descriptions for VP/Director of AI, AI Delivery, Digital Transformation
- Common required skills, certifications, and experience patterns
- What differentiates a $250K role from a $400K+ role at this level?
- The "strategy + engineering" unicorn — how rare is this profile and how is it described in job specs?

**Recruiter behavior:**
- Boolean search strings recruiters use for this profile type
- LinkedIn keywords that matter most for VP/AI search visibility
- Which recruiters and search firms are most active in this space?
- How candidates at this level are typically sourced (search firms, referrals, LinkedIn, conferences)

**Compensation landscape:**
- Director/VP AI compensation ranges (base, bonus, equity) — by industry and geography
- Consulting firm Director vs. enterprise VP total comp comparison
- How compensation is changing as AI leadership demand grows

### Output Format
Structured findings document with:
- 10+ specific job titles and their market positioning
- Keyword analysis from real job specs (what appears most frequently)
- Compensation ranges by role type and industry
- 3–5 active search firms specializing in this space
- Recommendations for LinkedIn keyword optimization based on actual search patterns
- The market narrative: where someone with Andrew's profile fits and what the path looks like

---

## How to Launch These Missions

Each mission is independent. Launch 3–4 agents simultaneously, then the remaining 3–4 once the first batch returns. Suggested batches:

**Batch 1 (highest priority — feeds the demos directly):**
- Mission 1: Enterprise Multi-Agent AI
- Mission 2: AI Governance
- Mission 4: Enterprise Prompt Engineering
- Mission 6: CRM Migration Deep Dive

**Batch 2 (feeds positioning and content):**
- Mission 3: Technical Due Diligence
- Mission 5: Conversion Triggers
- Mission 7: Director → VP Market

Each agent should return a structured document with the sections specified in "Output Format." Cite sources where possible. Prioritize 2025–2026 data over older material.

---

*Research brief created: February 2026*
